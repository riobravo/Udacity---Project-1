{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the NYC Subway Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project consists of two parts. In Part 1 of the project, you should have completed the questions in Problem Sets 2, 3, 4, and 5 in the Introduction to Data Science course.\n",
    "\n",
    "This document addresses part 2 of the project. Please use this document as a template and answer the following questions to explain your reasoning and conclusion behind your work in the problem sets. You will attach a document with your answers to these questions as part of your final project submission.\n",
    "\n",
    "## Section 0 - References\n",
    "\n",
    "- http://matplotlib.org/\n",
    "- http://pandas.pydata.org/\n",
    "    - Book: Python for Data Analysis (Wes McKinney)\n",
    "- http://stanford.edu/~mwaskom/software/seaborn/\n",
    "- http://ipython.org/\n",
    "    - http://nbviewer.ipython.org/github/yenlung/ipython/blob/master/examples/Interactive%20Widgets/Index.ipynb\n",
    "- http://www.scipy.org/\n",
    "    - http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html\n",
    "    - http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "- http://pyvideo.org/\n",
    "    - http://pyvideo.org/video/3395/pandas-from-the-ground-up\n",
    "    - http://pyvideo.org/speaker/1274/benjamin-root\n",
    "- https://www.python.org/\n",
    "    - https://docs.python.org/2/library/random.html?highlight=shuffle#random.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from random import shuffle\n",
    "from scipy.stats import mannwhitneyu\n",
    "from matplotlib import pyplot as plt, ticker as tkr\n",
    "from IPython.html.widgets import interact\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format Pandas DataFrame\n",
    "css = open('styles/table.css').read() + open('styles/notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to format y-axis and x-axis numbers\n",
    "def func(axis, pos):\n",
    "    s = '{:2,d}'.format(int(axis))\n",
    "    return s\n",
    " \n",
    "axis_format = tkr.FuncFormatter(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the csv file and put it in a Pandas DataFrame\n",
    "df = pd.read_csv('turnstile_weather_v2.csv', index_col='datetime', parse_dates=True)\n",
    "df = df.drop(['DATEn', 'TIMEn', 'weekday'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Statistical Test\n",
    "\n",
    "##### 1.1 - Which statistical test did you use to analyze the NYC subway data? Did you use a one-tail or a two-tail P value? What is the null hypothesis? What is your p-critical value?\n",
    "\n",
    "I have used the Mann-Whitney U Test and a two-sided test. The null hypothesis is that there is no difference between the two populations (rain vs no_rain averages for ENTRIESn_hourly), which would indicate that rain has no effect in ridership. The p-critical used was 5%.\n",
    "\n",
    "##### 1.2 - Why is this statistical test applicable to the dataset? In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.\n",
    "\n",
    "As per the Histogram of MTA Ridership below, neither rain or no_rain days are normally distributed, that is why I did not use the Welch's t-test, because it assumes that the populations are normally distributed and with equal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "def get_data():\n",
    "    rain = df[df['rain'] == 1]['ENTRIESn_hourly']\n",
    "    no_rain = df[df['rain'] == 0]['ENTRIESn_hourly']\n",
    "    return rain, no_rain\n",
    "\n",
    "# Plot the histogram\n",
    "def get_hist():\n",
    "    rain, no_rain = get_data()\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(no_rain, bins=50, label = 'NO_RAIN', color='g')\n",
    "    ax.hist(rain, bins=50, label = 'RAIN', color='b')\n",
    "    ax.set_xlim(right=15000)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_xlabel('ENTIRESn_hourly', fontsize=12)\n",
    "    ax.set_title(\"MTA Ridership - Rain vs No_Rain Days\", fontweight='bold', fontsize=15)\n",
    "    plt.gca().yaxis.set_major_formatter(axis_format)\n",
    "    plt.gca().xaxis.set_major_formatter(axis_format)\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "get_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 - What results did you get from this statistical test? These should include the following numerical values: p-values, as well as the means for each of the two samples under test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mann_whitney():\n",
    "    rain, no_rain = get_data()\n",
    "    U, p = mannwhitneyu(rain, no_rain)\n",
    "    print 'Average ridership for rain days: {0} \\nAverage ridership for no_rain days: {1}'.format(rain.mean(), \\\n",
    "                                                                                no_rain.mean()); print' '\n",
    "    print 'Mann-Whitney U Test: {0} \\nTwo-tail p-value: {1:f}'.format(U, 2 * p)\n",
    "    \n",
    "mann_whitney()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 - What is the significance and interpretation of these results?\n",
    "\n",
    "Because the p-value is much lower than the p-critical, I can reject the null hypothesis and say that ridership is influenced by rain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Linear Regression\n",
    "\n",
    "#### 2.1 - What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model:\n",
    "\n",
    "I have used Gradient Descent in my analysis.\n",
    "\n",
    "\n",
    "#### 2.2 - What features (input variables) did you use in your model? Did you use any dummy variables as part of your features?\n",
    "\n",
    "I have used fog, rain, tempi, wspdi, meanprecipi as my model's features. Unfortunately, I could not use a dummy variable in my model, because I had problems with the Pandas' join method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_features(df):\n",
    "    mu = df.mean()\n",
    "    sigma = df.std()\n",
    "    \n",
    "    if (sigma == 0).any():\n",
    "        raise Exception(\"One or more features had the same value for all samples, and thus could \" + \\\n",
    "                         \"not be normalized. Please do not include features with only a single value \" + \\\n",
    "                         \"in your model.\")\n",
    "    df_normalized = (df - df.mean()) / df.std()\n",
    "\n",
    "    return df_normalized, mu, sigma\n",
    "\n",
    "\n",
    "def compute_cost(predicted_values, values, theta):\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = np.square(predicted_values - values).sum()\n",
    "    cost = sum_of_square_errors / (2 * m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def calculate_predicted_values(features, theta):\n",
    "    predicted_values = np.dot(features, theta)\n",
    "\n",
    "    return predicted_values\n",
    "\n",
    "\n",
    "def calculate_theta(alpha, features, predicted_values, theta, values):\n",
    "    m = len(values) * 1.0\n",
    "    theta -= (alpha / m) * np.dot((predicted_values - values), features)\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        predicted_values = calculate_predicted_values(features, theta)\n",
    "        theta = calculate_theta(alpha, features, predicted_values, theta, values)\n",
    "        cost = compute_cost(predicted_values, values, theta)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return theta, pd.Series(cost_history)\n",
    "\n",
    "\n",
    "def predictions(dataframe):\n",
    "    features = dataframe[['fog', 'rain', 'tempi', 'wspdi', 'meanprecipi']]\n",
    "    dummy_units = pd.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "#     features = features.join(dummy_units)\n",
    "    values = dataframe['ENTRIESn_hourly']\n",
    "    m = len(values)\n",
    "    features, mu, sigma = normalize_features(features)\n",
    "    features['ones'] = np.ones(m)\n",
    "    features_array = np.array(features)\n",
    "    values_array = np.array(values)\n",
    "    alpha = 0.1\n",
    "    num_iterations = 50\n",
    "    theta_gradient_descent = np.zeros(len(features.columns))\n",
    "    theta_gradient_descent, cost_history = gradient_descent(features_array, \n",
    "                                                            values_array, \n",
    "                                                            theta_gradient_descent, \n",
    "                                                            alpha, \n",
    "                                                            num_iterations)\n",
    "    \n",
    "\n",
    "    plot = plot_cost_history(alpha, cost_history)\n",
    "    predictions = np.dot(features_array, theta_gradient_descent)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def plot_cost_history(alpha, cost_history):\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(range(len(cost_history)), cost_history, color='b')\n",
    "    ax.set_ylabel('Cost History', fontsize=12)\n",
    "    ax.set_xlabel('Iterations', fontsize=12)\n",
    "    ax.set_title('Cost History for Alpha = %.2f' % alpha, fontweight='bold', fontsize=15)\n",
    "    plt.gca().yaxis.set_major_formatter(axis_format)\n",
    "    plt.show()\n",
    "\n",
    "my_pred = predictions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Why did you select these features in your model? We are looking for specific reasons that lead you to believe that the selected features will contribute to the predictive power of your model.\n",
    "\n",
    "- Your reasons might be based on intuition. For example, response for fog might be: “I decided to use fog because I thought that when it is very foggy outside people might decide to use the subway more often.”\n",
    "- Your reasons might also be based on data exploration and experimentation, for example: “I used feature X because as soon as I included it in my model, it drastically improved my R2 value.”  \n",
    "\n",
    "#### 2.4 - What are the coefficients (or weights) of the non-dummy features in your linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 - What is your model’s R2 (coefficients of determination) value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def r_squared(df, predictions):\n",
    "    r_squared = 1 - np.sum((np.square(df['ENTRIESn_hourly'] - predictions))) / \\\n",
    "                np.sum(np.square(df['ENTRIESn_hourly'] - np.mean(df['ENTRIESn_hourly'])))\n",
    "    \n",
    "    return r_squared\n",
    "    \n",
    "r_squared(df, my_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 - What does this R2 value mean for the goodness of fit for your regression model? Do you think this linear model to predict ridership is appropriate for this dataset, given this R2  value?\n",
    "\n",
    "The R squared for the GD is very low, which means I can not explain much about of the data variability with the model and the features I have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_residuals(df, predictions):\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist((df['ENTRIESn_hourly'] - predictions), bins=100, color='b')\n",
    "    ax.set_xlim(left=-5000, right=15000)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_xlabel('Residuals', fontsize=12)\n",
    "    ax.set_title('Gradient Descent Residuals', fontweight='bold', fontsize=15)\n",
    "    plt.gca().xaxis.set_major_formatter(axis_format)\n",
    "    plt.gca().yaxis.set_major_formatter(axis_format)\n",
    "    plt.show()\n",
    "\n",
    "plot_residuals(df, my_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Visualization\n",
    "\n",
    "Please include two visualizations that show the relationships between two or more variables in the NYC subway data. Remember to add appropriate titles and axes labels to your plots. Also, please add a short description below each figure commenting on the key insights depicted in the figure.\n",
    "\n",
    "#### 3.1 - One visualization should contain two histograms: one of  ENTRIESn_hourly for rainy days and one of ENTRIESn_hourly for non-rainy days.\n",
    "\n",
    "- You can combine the two histograms in a single plot or you can use two separate plots.\n",
    "\n",
    "- If you decide to use to two separate plots for the two histograms, please ensure that the x-axis limits for both of the plots are identical. It is much easier to compare the two in that case.\n",
    "\n",
    "- For the histograms, you should have intervals representing the volume of ridership (value of ENTRIESn_hourly) on the x-axis and the frequency of occurrence on the y-axis. For example, each interval (along the x-axis), the height of the bar for this interval will represent the number of records (rows in our data) that have ENTRIESn_hourly that falls in this interval.\n",
    "\n",
    "- Remember to increase the number of bins in the histogram (by having larger number of bars). The default bin width is not sufficient to capture the variability in the two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the top 5 stations, calculated as the median of the 5 largest ENTRIESn_hourly\n",
    "station_group = df.groupby(['station'], as_index=False)\n",
    "ordered = station_group['station', 'ENTRIESn_hourly'].median().sort('ENTRIESn_hourly')\n",
    "top_stations = list(ordered.tail()['station'])\n",
    "\n",
    "# Get the data\n",
    "def get_station_data(station_name):\n",
    "    station_rain = df[(df[\"station\"] == station_name) & (df[\"rain\"] == 1)][\"ENTRIESn_hourly\"]\n",
    "    station_no_rain = df[(df[\"station\"] == station_name) & (df[\"rain\"] == 0)][\"ENTRIESn_hourly\"]\n",
    "    return station_rain, station_no_rain\n",
    "\n",
    "# Create a histogram chart comparing ridership on rain vs no_rain days\n",
    "def get_station_hist(station_name):\n",
    "    station_rain, station_no_rain = get_station_data(station_name)\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(station_no_rain, bins=20, label = 'NO_RAIN', color = 'g')\n",
    "    ax.hist(station_rain, bins=20, label = 'RAIN', color = 'b')\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_xlabel('Median ENTRIESn_hourly', fontsize=12)\n",
    "    ax.set_title('Ridership at ' + station_name + ' Station', fontweight='bold', fontsize=15)\n",
    "    plt.gca().xaxis.set_major_formatter(axis_format)\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Interact the chart\n",
    "i1 = interact(get_station_hist, station_name=top_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - One visualization can be more freeform. You should feel free to implement something that we discussed in class (e.g., scatter plots, line plots) or attempt to implement something more advanced if you'd like. Some suggestions are:\n",
    "\n",
    "- Ridership by time-of-day\n",
    "\n",
    "- Ridership by day-of-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select 20 stations randomly\n",
    "station_list = list(df.station.unique())\n",
    "shuffle(station_list)\n",
    "rand20_stations = station_list[0:20]\n",
    "\n",
    "# Get the data\n",
    "def get_rain_data(station_name):\n",
    "    rain = df[(df[\"station\"] == station_name) & (df[\"rain\"] == 1)]\n",
    "    rain_counts = rain.groupby('day_week')['ENTRIESn_hourly'].sum()\n",
    "    \n",
    "    no_rain = df[(df[\"station\"] == station_name) & (df[\"rain\"] == 0)]\n",
    "    no_rain_counts = no_rain.groupby('day_week')['ENTRIESn_hourly'].sum()\n",
    "\n",
    "    return rain_counts, no_rain_counts\n",
    "\n",
    "# Create a bar chart comparing ridership on rain vs no_rain days by weekday\n",
    "def get_rain_bar(station_name):\n",
    "    rain_counts, no_rain_counts = get_rain_data(station_name)\n",
    "    df = pd.merge(rain_counts.reset_index(0), no_rain_counts.reset_index(0), how='outer', left_on='day_week', \n",
    "                  right_on='day_week')\n",
    "    df = df.sort('day_week')\n",
    "    df = df.set_index('day_week')\n",
    "    df = df.rename(index={0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday',\n",
    "                          5: 'Saturday', 6: 'Sunday'}, columns={'ENTRIESn_hourly_x': 'RAIN', \n",
    "                                                                'ENTRIESn_hourly_y': 'NO_RAIN'})\n",
    "\n",
    "    df.plot(kind='bar', figsize=plt.figaspect(0.5), color=['b', 'g'])\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('ENTRIESn_hourly', fontsize=12)\n",
    "    plt.xlabel('Weekday', fontsize=12)\n",
    "    plt.title('Ridership by Weekday at ' + station_name + ' Station', fontweight='bold', fontsize=15)\n",
    "    plt.gca().yaxis.set_major_formatter(axis_format)\n",
    "    plt.show()\n",
    "\n",
    "# Interact the chart\n",
    "i2 = interact(get_rain_bar, station_name=rand20_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 4 - Conclusion\n",
    "\n",
    "Please address the following questions in detail. Your answers should be 1-2 paragraphs long.\n",
    "\n",
    "#### 4.1 - From your analysis and interpretation of the data, do more people ride the NYC subway when it is raining or when it is not raining?\n",
    "\n",
    "Particularly given the results from the Mann-Whitney U test (very low p-value), we can say with a high level of certainty that more people ride the NYC subway when it is raining. It is important to note that simply looking at the means of both data sets is insufficient, due to variance. The Mann-Whitney U test is needed to quantitatively confirm that the two data sets are statistically different.\n",
    "\n",
    "#### 4.2 - What analyses lead you to this conclusion? You should use results from both your statistical tests and your linear regression to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Reflection\n",
    "\n",
    "Please address the following questions in detail. Your answers should be 1-2 paragraphs long.\n",
    "\n",
    "#### 5.1 - Please discuss potential shortcomings of the methods of your analysis, including: Dataset, Analysis, such as the linear regression model or statistical test.\n",
    "\n",
    "I believe my model did not work well because I was not able to include a dummy variable to it.\n",
    "\n",
    "\n",
    "#### 5.2 - (Optional) Do you have any other insight about the dataset that you would like to share with us?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
